  services:
  # When scaling the opal-server to multiple nodes and/or multiple workers, we use
  # a *broadcast* channel to sync between all the instances of opal-server.
  # Under the hood, this channel is implemented by encode/broadcaster (see link below).
  # At the moment, the broadcast channel can be either: postgresdb, redis or kafka.
  # The format of the broadcaster URI string (the one we pass to opal server as `OPAL_BROADCAST_URI`) is specified here:
  # https://github.com/encode/broadcaster#available-backends
  broadcast_channel:
    image: postgres:alpine
    environment:
      - POSTGRES_DB=postgres
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
  opal_server:
    # by default we run opal-server from latest official image
    image: permitio/opal-server:latest
    #    deploy:
    #      mode: replicated
    #      replicas: 2
    #      endpoint_mode: vip
    environment:
      # the broadcast backbone uri used by opal server workers (see comments above for: broadcast_channel)
      - OPAL_BROADCAST_URI=postgres://postgres:postgres@broadcast_channel:5432/postgres
      # number of uvicorn workers to run inside the opal-server container
      - UVICORN_NUM_WORKERS=4
      # the git repo hosting our policy
      # - if this repo is not public, you can pass an ssh key via `OPAL_POLICY_REPO_SSH_KEY`)
      # - the repo we pass in this example is *public* and acts as an example repo with dummy rego policy
      # - for more info, see: https://docs.opal.ac/tutorials/track_a_git_repo
      - OPAL_POLICY_REPO_URL=https://github.com/permitio/opal-example-policy-repo
      # in this example we will use a polling interval of 30 seconds to check for new policy updates (git commits affecting the rego policy).
      # however, it is better to utilize a git *webhook* to trigger the server to check for changes only when the repo has new commits.
      # for more info see: https://docs.opal.ac/tutorials/track_a_git_repo
      - OPAL_POLICY_REPO_POLLING_INTERVAL=30
      # configures from where the opal client should initially fetch data (when it first goes up, after disconnection, etc).
      # the data sources represents from where the opal clients should get a "complete picture" of the data they need.
      # after the initial sources are fetched, the client will subscribe only to update notifications sent by the server.
      - OPAL_DATA_CONFIG_SOURCES={"config":{"entries":[{"url":"http://opal_server:7002/policy-data","config":{"headers":{"Authorization":"Bearer ${OPAL_CLIENT_TOKEN}"}},"topics":["policy_data"],"dst_path":"/static"}]}}
      - OPAL_LOG_FORMAT_INCLUDE_PID=true
      # Webhook configuration
      # - To simulate webhook, run the following command:
      #   curl --request POST 'http://localhost:7002/webhook' --header 'Content-Type: application/json' --header 'x-webhook-token: xxxxx' --data-raw '{"gitEvent":"git.push","repository":{"git_url":"https://github.com/permitio/opal-example-policy-repo"}}'
      - OPAL_POLICY_REPO_WEBHOOK_SECRET=xxxxx
      - OPAL_POLICY_REPO_WEBHOOK_PARAMS={"secret_header_name":"x-webhook-token","secret_type":"token","secret_parsing_regex":"(.*)","event_request_key":"gitEvent","push_event_value":"git.push"}

      # server secure mode
      # in order to run in "secure mode", meaning OPAL server will authenticate all API requests
      # by requiring a bearer token containing a valid OPAL JWT (all such JWTs are signed by the
      # OPAL server), you must provide a public key (used for verifying the JWT signature) and a
      # private key (used for signing on new JWT tokens).
      - OPAL_AUTH_PUBLIC_KEY=${OPAL_AUTH_PUBLIC_KEY}
      - OPAL_AUTH_PRIVATE_KEY=${OPAL_AUTH_PRIVATE_KEY}
      # the master token is used in only one scenario - when we want to generate a new JWT token.
      # the /token api endpoint on the OPAL server is the only endpoint that requires the master token.
      - OPAL_AUTH_MASTER_TOKEN=${OPAL_AUTH_MASTER_TOKEN}
      # --------------------------------------------------------------------------------
      # the jwt audience and jwt issuer are not typically necessary in real setups
      # --------------------------------------------------------------------------------
      # we recently changed issuers, this is just a precaution to make sure the
      # ./run-example-with-security.sh script uses the same issuer to issue JWT tokens
      # as the opal server that checks the tokens in the docker compose setup
      - OPAL_AUTH_JWT_AUDIENCE=https://api.opal.ac/v1/
      - OPAL_AUTH_JWT_ISSUER=https://opal.ac/

      # turning on statistics reporting on the client side
      - OPAL_STATISTICS_ENABLED=true
    ports:
      # exposes opal server on the host machine, you can access the server at: http://localhost:7002
      - "7002:7002"
    depends_on:
      - broadcast_channel
  opal_client:
    # by default we run opal-client from latest official image
    image: permitio/opal-client:latest
    environment:
      - OPAL_SERVER_URL=http://opal_server:7002
      - OPAL_LOG_FORMAT_INCLUDE_PID=true
      - OPAL_INLINE_OPA_LOG_FORMAT=http
      # update callbacks config ----------------------------------
      # this var turns on a callback (HTTP call to a configurable url) after every successful data update
      # and allows you to track which data updates completed successfully and were saved to OPA cache.
      - OPAL_SHOULD_REPORT_ON_DATA_UPDATES=True
      # we configure a callback to go to a default location in the OPAL server, but you can configure
      # a callback to any url you'd like. Each callback is either the url alone, or a tuple of
      # (url, HttpFetcherConfig).
      # We show here both ways to configure the same endpoint, one of them demonstrate how to
      # add extra HTTP headers (the header shown is ignored, only here for example).
      - OPAL_DEFAULT_UPDATE_CALLBACKS={"callbacks":[["http://opal_server:7002/data/callback_report",{"method":"post","process_data":false,"headers":{"Authorization":"Bearer ${OPAL_CLIENT_TOKEN}","content-type":"application/json"}}]]}
      # - OPAL_DEFAULT_UPDATE_CALLBACKS={"callbacks":[("http://opal_server:7002/data/callback_report",{"headers":{"X-My-Token":"token"}})]}
      # OPAL can load a special policy into OPA that acts as a healthcheck policy (Not directly related to the callback feature).
      # This policy defines two opa rules you can query:
      # ready rule (POST http://localhost:8181/data/system/opal/ready): signals that OPA is ready to accept authorization queries.
      # healthy rule (POST http://localhost:8181/data/system/opal/ready): signals that the last policy and data updates succeeded.
      - OPAL_OPA_HEALTH_CHECK_POLICY_ENABLED=True
      # end of update callbacks config ---------------------------

      - OPAL_CLIENT_TOKEN=${OPAL_CLIENT_TOKEN}
      # --------------------------------------------------------------------------------
      # the jwt audience and jwt issuer are not typically necessary in real setups
      # --------------------------------------------------------------------------------
      # we recently changed issuers, this is just a precaution to make sure the
      # ./run-example-with-security.sh script uses the same issuer to issue JWT tokens
      # as the opal server that checks the tokens in the docker compose setup
      - OPAL_AUTH_JWT_AUDIENCE=https://api.opal.ac/v1/
      - OPAL_AUTH_JWT_ISSUER=https://opal.ac/

      # turning on statistics collection on the server side
      - OPAL_STATISTICS_ENABLED=true
    ports:
      # exposes opal client on the host machine, you can access the client at: http://localhost:7766
      - "7766:7000"
      # exposes the OPA agent (being run by OPAL) on the host machine
      # you can access the OPA api that you know and love at: http://localhost:8181
      # OPA api docs are at: https://www.openpolicyagent.org/docs/latest/rest-api/
      - "8181:8181"
    depends_on:
      - opal_server
    # this command is not necessary when deploying OPAL for real, it is simply a trick for dev environments
    # to make sure that opal-server is already up before starting the client.
    command: sh -c "exec ./wait-for.sh opal_server:7002 --timeout=20 -- ./start.sh"
